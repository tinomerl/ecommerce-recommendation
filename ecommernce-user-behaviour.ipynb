{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Predicting User behaviour based on e-commerce data\n",
    "*__Author:__ Tino Merl*\n",
    "\n",
    "__Table of Contents__\n",
    "\n",
    "* [Introduction and planned action](#intro)\n",
    "    * [CRISP-DM](#crisp)\n",
    "* [Business Understanding](#bus_und)\n",
    "* [Data Understanding](#dat_und)\n",
    "    * [Describing the columns](#col_descr)\n",
    "    * [Describing the files](#fil_descr)\n",
    "* [Data Preparation](#dat_pre)\n",
    "* [Modeling](#model)\n",
    "* [Evaluation](#eval)\n",
    "\n",
    "## Introduction and planned action<a class=\"anchor\" id=\"intro\"></a>\n",
    "This is an assignment for the module applied programming in the summerterm of 2020 at the *FOM Hochschule für Oekonomie & Management* at the study center in cologne. Troughout this assignment i will work with a dataset which contains user data from an e-commerce system. The dataset can be found on kaggle named *Retailrocket recommender system dataset*.[[1](#kaggle_dataset)] The dataset contains four individual files. Since two of them (item_properties_part1 and item_properties_part2) exceed the maximum filesize allowed on github i am not able to upload them to this repository. The goal of this assignment is to predict user behaviour. This can be done in two ways. The Users can be clustered in a way to predict whether a user contains to a group that buys or not. It can also be done by using markov chains to calculate the probability of a user buying or not. This whole analysis will be done via the CRISP-DM Process. \n",
    "\n",
    "### CRISP-DM<a class=\"anchor\" id=\"crisp\"></a>\n",
    "CRISP-DM stands for __C__ross __I__ndustry __S__tandard __P__rocess for __D__ata __M__ining. It is a standardized process which describes the steps a machine learning analysis and model building should undertake. The steps are the following six.\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "They may be listed in a sequential manner, but there is a lot of back and forth between the steps. Especially between Business Understanding and Data Understanding, Business Understanding and Evaluation such as Data Preparation and modeling. Figure 1 illustrates the circular nature of the process.\n",
    "\n",
    "<div style=\"margin:auto;\">\n",
    "<img style=\"display:block; margin-left: auto; margin-right:auto;\" src=\"img/crisp-dm_diagramm.png\"/>\n",
    "<div style=\"width: 50%; margin:0 auto; text-align:center;\"><i><b>Figure 1:</b></i> CRISP-DM diagram by statistik-dresden.de[<a href=\"#crisp-dm_diagramm\">2</a>]</div>\n",
    "</div>\n",
    "\n",
    "Since this is an assignment the last step, the deployment, will be left out. We will therefore end the process with step number five: evaluation.\n",
    "\n",
    "## Business Understanding<a class=\"anchor\" id=\"bus_und\"></a>\n",
    "\n",
    "The first step is the business understanding. In this step the concrete goals and requirements are set before the analysis begins. The concrete tasks will also be defined here.[[2](#crisp-dm_diagramm)] For this assignment the concrete tasks will be the following.\n",
    "\n",
    "* *Explorative analysis of the data in data understanding*\n",
    "* *Can the user behaviour be predicted with a simple clustering algorithm?*\n",
    "* *Can the user behaviour be predicted with markov chains?*\n",
    "* *Which of the models performs better?*\n",
    "\n",
    "Tasks may be target of changes and additions.\n",
    "\n",
    "## Data Understanding<a class=\"anchor\" id=\"dat_und\"></a>\n",
    "\n",
    "As the next step is the data understanding we should usually try to understand the data by talking with stakeholders and data owners. This is then followed by an explorative analysis of the dataset, which also creates the foundation for the following chapter the data preparation. Since this dataset has a usability score of 8.8 kaggle and has also a lot of context describing the dataset i will cite the kaggle page.[[1](#kaggle_dataset)]\n",
    "\n",
    "### Describing the files<a class=\"anchor\" id=\"fil_descr\"></a>\n",
    "\n",
    "### Describing the columns<a class=\"anchor\" id=\"col_descr\"></a>\n",
    "\n",
    "The following descriptions are taken from the kaggle dataset.[[1](#kaggle_dataset)]\n",
    "\n",
    "__category_tree.csv__\n",
    "\n",
    "* categoryid `{int}` -- unique identifier of the category.\n",
    "* parentid `{int}` -- identifier of the parent category. It's empty, if parent doesn't exist.\n",
    "\n",
    "__events.csv__\n",
    "\n",
    "* timestamp `{int}` -- the time, when event is occurred, in milliseconds since 01-01-1970.\n",
    "* visitorid `{int}` -- unique identifier of the visitor\n",
    "* event `{string}` -- type of the event {“view”, “addtocart”, “transaction”}\n",
    "* itemid `{int}` -- unique identifier of the item\n",
    "* transactionid `{int}` -- unique identifier of the transaction (non empty only for transaction event type).\n",
    "\n",
    "__item_properties_part1.csv__\n",
    "\n",
    "* timestamp `{int}` -- snapshot creation time (Unix timestamp in milliseconds)\n",
    "* itemid `{int}` -- unique Id of the item\n",
    "* property `{str}` -- property of the Item. All of them had been hashed excluding \"categoryid\" and \"available\"\n",
    "* value `{str}` -- property value of the item\n",
    "\n",
    "__item_properties_part1.csv__\n",
    "\n",
    "* timestamp `{int}` -- snapshot creation time (Unix timestamp in milliseconds)\n",
    "* itemid `{int}` -- unique Id of the item\n",
    "* property `{str}` -- property of the Item. All of them had been hashed excluding \"categoryid\" and \"available\"\n",
    "* value `{str}` -- property value of the item\n",
    "\n",
    "\n",
    "__Loading of the needed Packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading of the datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryTreeDf = pd.read_csv(\"./data/category_tree.csv\")\n",
    "eventsDf = pd.read_csv(\"./data/events.csv\")\n",
    "properties1Df = pd.read_csv(\"./data/item_properties_part1.csv\")\n",
    "properties2Df = pd.read_csv(\"./data/item_properties_part2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratives Betrachten der Dataframes als ganzes. Columns und Heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['categoryid', 'parentid'], dtype='object')\n   categoryid  parentid\n0        1016     213.0\n1         809     169.0\n2         570       9.0\n3        1691     885.0\n4         536    1691.0\n\n\n\nIndex(['timestamp', 'visitorid', 'event', 'itemid', 'transactionid'], dtype='object')\n       timestamp  visitorid event  itemid  transactionid\n0  1433221332117     257597  view  355908            NaN\n1  1433224214164     992329  view  248676            NaN\n2  1433221999827     111016  view  318965            NaN\n3  1433221955914     483717  view  253185            NaN\n4  1433221337106     951259  view  367447            NaN\n\n\n\nIndex(['timestamp', 'itemid', 'property', 'value'], dtype='object')\n       timestamp  itemid    property                            value\n0  1435460400000  460429  categoryid                             1338\n1  1441508400000  206783         888          1116713 960601 n277.200\n2  1439089200000  395014         400  n552.000 639502 n720.000 424566\n3  1431226800000   59481         790                       n15360.000\n4  1431831600000  156781         917                           828513\n\n\n\nIndex(['timestamp', 'itemid', 'property', 'value'], dtype='object')\n       timestamp  itemid property            value\n0  1433041200000  183478      561           769062\n1  1439694000000  132256      976  n26.400 1135780\n2  1435460400000  420307      921  1149317 1257525\n3  1431831600000  403324      917          1204143\n4  1435460400000  230701      521           769062\n\n\n\n"
    }
   ],
   "source": [
    "for elem in [\n",
    "    categoryTreeDf,\n",
    "    eventsDf,\n",
    "    properties1Df,\n",
    "    properties2Df,\n",
    "    ]:\n",
    "\n",
    "    print(elem.columns)\n",
    "    print(\"\\n\")\n",
    "    print(elem.head())\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes\n",
    "[1]<a class=\"anchor\" id=\"kaggle_dataset\"></a> Retailrocket (2017) Retailrocket recommender system dataset, Version 4. Retrieved 2020-04-19 from https://www.kaggle.com/retailrocket/ecommerce-dataset\n",
    "\n",
    "[2]<a class=\"anchor\" id=\"crisp-dm_diagramm\"></a> Wolf Riepel (2012). CRISP-DM: Ein Standard-Prozess-Modell für Data Mining. Retrieved 2020-05-10 from https://statistik-dresden.de/archives/1128"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitvenvvenv040c9b7a2346479db7d5967906aaf6a0",
   "display_name": "Python 3.7.5 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}